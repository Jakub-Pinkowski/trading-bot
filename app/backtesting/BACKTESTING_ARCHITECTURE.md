# Backtesting Architecture

## Overview

The backtesting system is designed to efficiently test multiple trading strategies across different time periods,
symbols, and intervals using parallel processing and intelligent caching. The architecture prioritizes reliability,
performance, and accurate trade simulation.

## System Flow

### 1. Mass Tester Initialization

**Entry Point**: `MassTester.__init__()` in `app/backtesting/testing/mass_tester.py`

```
Initialize MassTester
â”œâ”€â”€ Load switch dates from YAML
â”‚   â””â”€â”€ Contract rollover dates for futures trading
â”œâ”€â”€ Store test parameters
â”‚   â”œâ”€â”€ tested_months: List of months to backtest
â”‚   â”œâ”€â”€ symbols: List of symbols (e.g., 'ZS', 'GC', 'CL')
â”‚   â””â”€â”€ intervals: List of timeframes (e.g., '1h', '4h', '1d')
â””â”€â”€ Initialize results storage
```

**Adding Strategies**:

```
MassTester.add_*_tests()
â”œâ”€â”€ Create parameter grid (all combinations)
â”‚   â””â”€â”€ Example: rsi_periods Ã— lower_thresholds Ã— upper_thresholds
â”œâ”€â”€ Generate strategy instances
â”‚   â””â”€â”€ Use strategy_factory.create_strategy()
â””â”€â”€ Store in self.strategies list
```

### 2. Test Execution Flow

**Main Execution**: `MassTester.run_tests()`

```
Phase 1: Preparation
â”œâ”€â”€ Load existing results from parquet
â”‚   â””â”€â”€ Create set of (month, symbol, interval, strategy) tuples for O(1) lookup
â”œâ”€â”€ Generate all test combinations
â”‚   â””â”€â”€ Cartesian product of: months Ã— symbols Ã— intervals Ã— strategies
â”œâ”€â”€ Preprocess switch dates
â”‚   â””â”€â”€ Convert to pandas datetime for each symbol
â”œâ”€â”€ Cache file paths
â”‚   â””â”€â”€ Build filepath patterns for each (month, symbol, interval)
â””â”€â”€ Filter already-run tests (if skip_existing=True)

Phase 2: Parallel Execution
â”œâ”€â”€ Create ProcessPoolExecutor with max_workers
â”œâ”€â”€ Submit all test combinations to worker pool
â”‚   â””â”€â”€ Each worker runs _run_single_test()
â”œâ”€â”€ Monitor progress
â”‚   â”œâ”€â”€ Print progress every 100 tests
â”‚   â”œâ”€â”€ Save intermediate results every 1000 tests
â”‚   â””â”€â”€ Run garbage collection periodically
â””â”€â”€ Handle worker exceptions gracefully
    â””â”€â”€ Log and continue (don't crash entire run)

Phase 3: Results Aggregation
â”œâ”€â”€ Collect results from completed futures
â”œâ”€â”€ Convert to DataFrame
â”‚   â”œâ”€â”€ Validate metrics types
â”‚   â””â”€â”€ Handle missing/invalid values
â”œâ”€â”€ Save to parquet (append mode)
â””â”€â”€ Save caches (DataFrame and Indicator)
```

### 3. Single Test Execution (Worker Process)

**Worker Function**: `MassTester._run_single_test()`

```
For Each Test Combination:
â”œâ”€â”€ 1. Load DataFrame
â”‚   â”œâ”€â”€ Check DataFrame cache first
â”‚   â”‚   â””â”€â”€ get_cached_dataframe(filepath)
â”‚   â”œâ”€â”€ If not cached, load from parquet
â”‚   â””â”€â”€ Validate DataFrame
â”‚       â”œâ”€â”€ Check required columns (open, high, low, close)
â”‚       â”œâ”€â”€ Check for excessive NaN values (>10%)
â”‚       â”œâ”€â”€ Verify index is sorted
â”‚       â””â”€â”€ Check for duplicate timestamps
â”‚
â”œâ”€â”€ 2. Run Strategy
â”‚   â””â”€â”€ strategy_instance.run(df, switch_dates)
â”‚       â”œâ”€â”€ 2a. Add Indicators
â”‚       â”‚   â”œâ”€â”€ Strategy calls indicator functions
â”‚       â”‚   â”‚   â””â”€â”€ calculate_rsi(), calculate_ema(), etc.
â”‚       â”‚   â”œâ”€â”€ Each indicator checks cache first
â”‚       â”‚   â”‚   â””â”€â”€ Hash input series + parameters
â”‚       â”‚   â”‚   â””â”€â”€ Return cached value if available
â”‚       â”‚   â””â”€â”€ Add indicator columns to DataFrame
â”‚       â”‚
â”‚       â”œâ”€â”€ 2b. Generate Signals
â”‚       â”‚   â”œâ”€â”€ Apply strategy logic to indicators
â”‚       â”‚   â”œâ”€â”€ Use helper methods:
â”‚       â”‚   â”‚   â”œâ”€â”€ _detect_crossover() for line crosses
â”‚       â”‚   â”‚   â””â”€â”€ _detect_threshold_cross() for threshold breaks
â”‚       â”‚   â””â”€â”€ Add 'signal' column to DataFrame
â”‚       â”‚       â”œâ”€â”€ 1 = Long entry signal
â”‚       â”‚       â”œâ”€â”€ -1 = Short entry signal
â”‚       â”‚       â””â”€â”€ 0 = No action
â”‚       â”‚
â”‚       â””â”€â”€ 2c. Extract Trades
â”‚           â”œâ”€â”€ Iterate through DataFrame row by row
â”‚           â”œâ”€â”€ Skip first INDICATOR_WARMUP_PERIOD candles (100)
â”‚           â”œâ”€â”€ Handle trailing stops (if enabled)
â”‚           â”œâ”€â”€ Handle contract switches (futures rollover)
â”‚           â”œâ”€â”€ Execute queued signals from previous bar
â”‚           â”‚   â””â”€â”€ See "Signal Queuing" section below
â”‚           â””â”€â”€ Return list of trades
â”‚
â”œâ”€â”€ 3. Calculate Metrics
â”‚   â”œâ”€â”€ Per-trade metrics (for each trade)
â”‚   â”‚   â””â”€â”€ calculate_trade_metrics(trade, symbol)
â”‚   â”‚       â”œâ”€â”€ Load contract specs (multiplier, margin)
â”‚   â”‚       â”œâ”€â”€ Calculate P&L in points and dollars
â”‚   â”‚       â”œâ”€â”€ Calculate percentage returns
â”‚   â”‚       â””â”€â”€ Add commission costs
â”‚   â”‚
â”‚   â””â”€â”€ Summary metrics (aggregate)
â”‚       â””â”€â”€ SummaryMetrics.calculate_all_metrics()
â”‚           â”œâ”€â”€ Basic: total_trades, win_rate
â”‚           â”œâ”€â”€ Returns: total/average returns (% of contract and margin)
â”‚           â”œâ”€â”€ Risk: profit_factor, max_drawdown
â”‚           â””â”€â”€ Advanced: Sharpe, Sortino, Calmar ratios, VaR, ES
â”‚
â””â”€â”€ 4. Return Result
    â””â”€â”€ Dictionary with:
        â”œâ”€â”€ month, symbol, interval, strategy
        â”œâ”€â”€ metrics (dictionary of all calculated metrics)
        â”œâ”€â”€ timestamp (ISO format)
        â””â”€â”€ verbose_output (if verbose=True)
```

### 4. Results Aggregation and Storage

**Post-Processing**: After all workers complete

```
Results Processing:
â”œâ”€â”€ Collect all worker results
â”œâ”€â”€ Convert to DataFrame
â”‚   â”œâ”€â”€ Pre-allocate arrays for efficiency
â”‚   â”œâ”€â”€ Validate metric types (must be numeric)
â”‚   â””â”€â”€ Handle inf/NaN values (replace with 0)
â”‚
â””â”€â”€ Save to Parquet
    â”œâ”€â”€ Filename: mass_test_results_all.parquet
    â”œâ”€â”€ Use save_to_parquet() with file locking
    â””â”€â”€ Append to existing results (unique entries)

Cache Management:
â”œâ”€â”€ Save DataFrame cache to disk
â”‚   â””â”€â”€ Prevents reloading same data in future runs
â””â”€â”€ Save Indicator cache to disk
    â””â”€â”€ Prevents recalculating same indicators
```

## Key Design Decisions

### Why Queue Signals?

**Problem**: In real trading, you can't execute at the exact moment a signal is generated.

**Solution**: Queue signals for next-bar execution.

```
Bar N (Signal Generated):
â”œâ”€â”€ Close price: 100
â”œâ”€â”€ Strategy detects crossover
â””â”€â”€ Signal queued: BUY

Bar N+1 (Signal Executed):
â”œâ”€â”€ Open price: 101
â””â”€â”€ Position opened at 101 (not 100)
```

**Implementation**:

- Signals are detected based on bar close data
- Signal stored in `self.queued_signal`
- Next bar, signal executed at open price
- This simulates realistic order execution delay

## Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Main Process                             â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Load Existingâ”‚      â”‚   Generate   â”‚      â”‚  Pre-process â”‚ â”‚
â”‚  â”‚   Results    â”‚ â”€â”€â”€> â”‚ Test Combos  â”‚ â”€â”€â”€> â”‚ Switch Dates â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                            â”‚          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                              â–¼                                  â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                  â”‚ ProcessPoolExecutor   â”‚                      â”‚
â”‚                  â”‚   (max_workers CPUs)  â”‚                      â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                              â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                   â”‚                   â”‚
           â–¼                   â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Worker 1 â”‚        â”‚ Worker 2 â”‚        â”‚ Worker N â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                   â”‚                   â”‚
           â”‚                   â”‚                   â”‚
    Each Worker Executes:                         
           â”‚                                       
           â”œâ”€> Load DataFrame (with cache)         
           â”‚                                       
           â”œâ”€> Add Indicators (with cache)         
           â”‚   â”œâ”€> Check indicator_cache           
           â”‚   â”œâ”€> Calculate if not cached         
           â”‚   â””â”€> Store in cache                  
           â”‚                                       
           â”œâ”€> Generate Signals                    
           â”‚   â”œâ”€> Apply strategy logic            
           â”‚   â””â”€> Queue signals for next bar      
           â”‚                                       
           â”œâ”€> Extract Trades                      
           â”‚   â”œâ”€> Skip warm-up period (100 bars) 
           â”‚   â”œâ”€> Execute queued signals          
           â”‚   â”œâ”€> Handle trailing stops           
           â”‚   â””â”€> Handle contract switches        
           â”‚                                       
           â”œâ”€> Calculate Metrics                   
           â”‚   â”œâ”€> Per-trade metrics               
           â”‚   â””â”€> Summary metrics                 
           â”‚                                       
           â””â”€> Return Result                       
                   â”‚                               
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        
                               â”‚          â”‚        
                               â–¼          â–¼        
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
                    â”‚   Results Collection   â”‚     
                    â”‚    (Main Process)      â”‚     
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     
                               â”‚                   
                               â–¼                   
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
                    â”‚ Convert to DataFrame   â”‚     
                    â”‚  Validate Metrics      â”‚     
                    â”‚  Save to Parquet       â”‚     
                    â”‚  Save Caches           â”‚     
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     
```

---

## Multi-Process Execution Model

### Overview

The backtesting system uses Python's `ProcessPoolExecutor` to distribute work across multiple CPU cores. Each worker
process runs in a separate memory space with its own Python interpreter.

### Process Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MAIN PROCESS                              â”‚
â”‚                          (PID: 12345)                               â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Responsibilities:                                        â”‚     â”‚
â”‚  â”‚  â€¢ Load existing results from parquet                     â”‚     â”‚
â”‚  â”‚  â€¢ Generate all test combinations                         â”‚     â”‚
â”‚  â”‚  â€¢ Create ProcessPoolExecutor                             â”‚     â”‚
â”‚  â”‚  â€¢ Submit tasks to worker pool                            â”‚     â”‚
â”‚  â”‚  â€¢ Monitor progress and save intermediate results         â”‚     â”‚
â”‚  â”‚  â€¢ Collect results from completed tasks                   â”‚     â”‚
â”‚  â”‚  â€¢ Merge worker caches (DataFrame + Indicator)            â”‚     â”‚
â”‚  â”‚  â€¢ Save final consolidated cache to disk                  â”‚     â”‚
â”‚  â”‚  â€¢ Save aggregated results to parquet                     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                     â”‚
â”‚  Memory State:                                                      â”‚
â”‚  â”œâ”€ DataFrame Cache: Loaded from disk at startup                   â”‚
â”‚  â”œâ”€ Indicator Cache: Loaded from disk at startup                   â”‚
â”‚  â”œâ”€ Results List: Accumulates results from workers                 â”‚
â”‚  â””â”€ Switch Dates: Pre-processed and passed to workers              â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ Creates executor with max_workers
                               â”‚
                               â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚    ProcessPoolExecutor               â”‚
            â”‚    (max_workers = CPU count)         â”‚
            â”‚                                      â”‚
            â”‚  Manages worker process lifecycle:    â”‚
            â”‚  â€¢ Spawns worker processes           â”‚
            â”‚  â€¢ Distributes tasks to workers      â”‚
            â”‚  â€¢ Collects results from workers     â”‚
            â”‚  â€¢ Handles worker exceptions         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                      â”‚                      â”‚
        â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WORKER 1     â”‚      â”‚  WORKER 2     â”‚     â”‚  WORKER N     â”‚
â”‚  (PID: 12346) â”‚      â”‚  (PID: 12347) â”‚ ... â”‚  (PID: 12350) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚
        â”‚                      â”‚                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                Each Worker Process:
                               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                     â”‚
    â”‚  Initialization (Once per worker):                  â”‚
    â”‚  â”œâ”€ Copy of main process memory at spawn           â”‚
    â”‚  â”œâ”€ Load DataFrame cache from disk                 â”‚
    â”‚  â”œâ”€ Load Indicator cache from disk                 â”‚
    â”‚  â””â”€ Independent Python interpreter                 â”‚
    â”‚                                                     â”‚
    â”‚  Processing Loop (For each assigned test):         â”‚
    â”‚  â”œâ”€ Receive test parameters from main              â”‚
    â”‚  â”œâ”€ Load DataFrame (check cache first)             â”‚
    â”‚  â”œâ”€ Run strategy (indicators auto-cache)           â”‚
    â”‚  â”œâ”€ Calculate metrics                              â”‚
    â”‚  â”œâ”€ Add computed indicators to local cache         â”‚
    â”‚  â””â”€ Return result to main process                  â”‚
    â”‚                                                     â”‚
    â”‚  Memory State (Isolated):                          â”‚
    â”‚  â”œâ”€ DataFrame Cache: Starts as copy, grows locally â”‚
    â”‚  â”œâ”€ Indicator Cache: Starts as copy, grows locally â”‚
    â”‚  â””â”€ Results: Returned to main via IPC              â”‚
    â”‚                                                     â”‚
    â”‚  Note: Memory is not shared with main process      â”‚
    â”‚        Cache updates remain in worker memory       â”‚
    â”‚                                                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Process Lifecycle

**Phase 1: Initialization**

```
Main Process:
  â”œâ”€ 1. Load caches from disk
  â”‚   â”œâ”€ dataframe_cache.pkl (previously cached DataFrames)
  â”‚   â””â”€ indicator_cache.pkl (previously cached indicators)
  â”‚
  â”œâ”€ 2. Create ProcessPoolExecutor
  â”‚   â””â”€ max_workers = os.cpu_count() (default: use all CPUs)
  â”‚
  â””â”€ 3. Submit all test combinations as tasks
      â””â”€ Each task = (month, symbol, interval, strategy)

Worker Processes (spawned by executor):
  â”œâ”€ 1. New Python process spawned
  â”‚   â””â”€ Copy-on-write: Initially shares memory with parent
  â”‚
  â”œâ”€ 2. Import modules
  â”‚   â””â”€ Each worker imports app.backtesting modules
  â”‚
  â””â”€ 3. Initialize caches
      â”œâ”€ Load dataframe_cache from disk (gets 100 entries)
      â””â”€ Load indicator_cache from disk (gets 500 entries)
```

**Phase 2: Parallel Execution**

```
Main Process:
  â”œâ”€ Monitor task completion
  â”œâ”€ Print progress every 100 tests
  â”œâ”€ Save intermediate results every 1000 tests
  â””â”€ Collect results as tasks complete

Worker 1:
  â”œâ”€ Process task 1: (202401, ZS, 1h, RSI_14_30_70)
  â”‚   â”œâ”€ Load DataFrame from cache (HIT) or disk (MISS)
  â”‚   â”œâ”€ Calculate RSI indicator
  â”‚   â”‚   â”œâ”€ Check indicator_cache (MISS - first time)
  â”‚   â”‚   â”œâ”€ Calculate RSI
  â”‚   â”‚   â””â”€ Store in local cache (501 entries now)
  â”‚   â”œâ”€ Generate signals and extract trades
  â”‚   â”œâ”€ Calculate metrics
  â”‚   â””â”€ Return result to main
  â”‚
  â”œâ”€ Process task 2: (202401, ZS, 1h, RSI_21_30_70)
  â”‚   â”œâ”€ Same DataFrame (cache HIT!)
  â”‚   â”œâ”€ Calculate RSI with period=21
  â”‚   â”‚   â”œâ”€ Check indicator_cache (MISS)
  â”‚   â”‚   â”œâ”€ Calculate RSI
  â”‚   â”‚   â””â”€ Store in local cache (502 entries)
  â”‚   â””â”€ Return result
  â”‚
  â””â”€ Continue processing assigned tasks...

Worker 2 (in parallel):
  â”œâ”€ Process task 3: (202401, GC, 1h, EMA_9_21)
  â”‚   â””â”€ Different symbol, different indicators
  â”‚       â””â”€ Local cache grows independently
  â”‚
  â””â”€ Continue processing assigned tasks...

Note: Worker cache updates are isolated
  â”œâ”€ Worker 1 cache: 600 entries (in Worker 1 memory only)
  â”œâ”€ Worker 2 cache: 450 entries (in Worker 2 memory only)
  â””â”€ Main cache: Still 500 entries (unchanged)
```

**Phase 3: Cleanup & Aggregation**

```
Main Process (after all workers complete):
  â”œâ”€ 1. All tasks finished
  â”‚   â””â”€ Workers terminate
  â”‚
  â”œâ”€ 2. Collect all results
  â”‚   â””â”€ Results passed via IPC (pickle serialization)
  â”‚
  â”œâ”€ 3. Convert to DataFrame
  â”‚   â””â”€ Validate metrics and handle NaN/inf values
  â”‚
  â”œâ”€ 4. Save results to parquet
  â”‚   â””â”€ Append to existing file (with file locking)
  â”‚
  â””â”€ 5. Save caches to disk
      â”œâ”€ dataframe_cache.pkl
      â””â”€ indicator_cache.pkl
```

### Cache Updates and Process Memory

Worker process cache updates remain in worker memory space and are not persisted when workers terminate.

### Inter-Process Communication (IPC)

**How Workers Communicate with Main**:

```
Worker Process                    Main Process
     â”‚                                 â”‚
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
     â”‚  â”‚  Process Test          â”‚     â”‚
     â”‚  â”‚  Calculate Metrics     â”‚     â”‚
     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
     â”‚              â”‚                  â”‚
     â”‚              â–¼                  â”‚
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
     â”‚  â”‚  Serialize Result      â”‚     â”‚
     â”‚  â”‚  (pickle)              â”‚     â”‚
     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
     â”‚              â”‚                  â”‚
     â”‚              â–¼                  â”‚
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
     â”‚  â”‚  Send via Pipe/Queue   â”‚     â”‚
     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
     â”‚              â”‚                  â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚              â–¼                  â”‚
     â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                   â”‚  Receive Result        â”‚
     â”‚                   â”‚  Deserialize (unpickle)â”‚
     â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                               â”‚
     â”‚                               â–¼
     â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                   â”‚  Append to Results List â”‚
     â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What Gets Serialized**:

- Test parameters (month, symbol, interval, strategy)
- Calculated metrics (dictionary)
- Verbose output (if enabled)
- Not serialized: DataFrame (too large)
- Not serialized: Cache objects (not needed)

### Process Pool Configuration

```python
# In testing/mass_tester.py

def run_tests(self, max_workers=None):
    """
    max_workers:
        - None: Use os.cpu_count() (all CPUs)
        - Integer: Specific number of workers
        - 1: Sequential (no multiprocessing)
    """
    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        futures = {
            executor.submit(self._run_single_test, params): params
            for params in test_combinations
        }

        # Process as they complete (not in submission order)
        for future in concurrent.futures.as_completed(futures):
            result = future.result()  # Blocks until result available
            if result:
                self.results.append(result)
```

**Typical Performance**:

| CPU Cores | Max Workers | Observed Speedup |
|-----------|-------------|------------------|
| 4         | 4           | 3.5x             |
| 8         | 8           | 7.0x             |
| 16        | 16          | 13x              |
| 4         | 1           | 1x (sequential)  |

**Overhead**:

- Process spawn time: ~0.5-1 second per worker
- IPC serialization: ~1-10ms per result
- Context switching: Minimal

---

## Cache Coordination Between Processes

### Cache Architecture

The system uses **two independent caches**:

1. **DataFrame Cache** - Stores loaded DataFrames to avoid re-parsing parquet files
2. **Indicator Cache** - Stores calculated indicators to avoid redundant calculations

Both use the same base architecture but serve different purposes.

### Cache Hierarchy

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Disk Storage (Persistent) â”‚
                    â”‚                             â”‚
                    â”‚  dataframe_cache.pkl        â”‚
                    â”‚  indicator_cache.pkl        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    Load at       â”‚       Save at
                    startup       â”‚       shutdown
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Main Process Memory      â”‚
                    â”‚                           â”‚
                    â”‚  dataframe_cache (50)     â”‚
                    â”‚  indicator_cache (500)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    Copy at       â”‚       No sync
                    worker spawn  â”‚       (isolated)
                                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                        â”‚                        â”‚
        â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker 1     â”‚        â”‚  Worker 2     â”‚      â”‚  Worker N     â”‚
â”‚               â”‚        â”‚               â”‚      â”‚               â”‚
â”‚  df_cache(50) â”‚        â”‚  df_cache(50) â”‚      â”‚  df_cache(50) â”‚
â”‚  ind_cache    â”‚        â”‚  ind_cache    â”‚      â”‚  ind_cache    â”‚
â”‚  (500)        â”‚        â”‚  (500)        â”‚      â”‚  (500)        â”‚
â”‚               â”‚        â”‚               â”‚      â”‚               â”‚
â”‚  Grows        â”‚        â”‚  Grows        â”‚      â”‚  Grows        â”‚
â”‚  independentlyâ”‚        â”‚  independentlyâ”‚      â”‚  independentlyâ”‚
â”‚  to 600       â”‚        â”‚  to 450       â”‚      â”‚  to 520       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                        â”‚                        â”‚
        â”‚                        â”‚                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                     When workers terminate:
                     Cache updates remain in worker memory
                     and are not saved back to disk
```

### Cache Implementation: DataFrame Cache

**Purpose**: Store parsed DataFrames to avoid re-reading parquet files.

```python
# In cache/dataframe_cache.py

from app.backtesting.cache.cache_base import Cache

# Singleton instance
dataframe_cache = Cache(
    cache_name="dataframes",
    max_size=50,  # Store up to 50 DataFrames
    max_age=604800  # 7 days TTL
)


# Usage in _run_single_test():
def _run_single_test(self, test_params):
    filepath = self.cache_file_paths[(symbol, interval)]

    # Try to get from cache
    df = dataframe_cache.get(filepath)

    if df is None:
        # Cache MISS - load from disk
        df = pd.read_parquet(filepath)
        # Store in cache for future use
        dataframe_cache.set(filepath, df)
    else:
        # Cache HIT - return immediately (no disk I/O)
        pass

    return df
```

**Cache Key**: File path (e.g., `/data/backtesting/202401_ZS_1h.parquet`)

**Behavior**:

- Multiple strategies test same (month, symbol, interval) combination
- DataFrame is identical for all strategies on that combination
- DataFrame loaded once per worker, reused for multiple tests

**Example**:

```
Worker 1 processes:
  â”œâ”€ Test 1: (202401, ZS, 1h, RSI_14_30_70)
  â”‚   â””â”€ Load ZS_1h DataFrame (cache MISS) â†’ Store in cache
  â”‚
  â”œâ”€ Test 2: (202401, ZS, 1h, RSI_21_30_70)
  â”‚   â””â”€ Load ZS_1h DataFrame (cache HIT) â†’ Return from memory
  â”‚
  â””â”€ Test 3: (202401, ZS, 1h, EMA_9_21)
      â””â”€ Load ZS_1h DataFrame (cache HIT) â†’ Return from memory

Timing:
  â”œâ”€ Disk read: ~50ms
  â””â”€ Cache read: ~0.1ms
```

### Cache Implementation: Indicator Cache

**Purpose**: Store calculated indicators to avoid redundant computations.

```python
# In cache/indicators_cache.py

from app.backtesting.cache.cache_base import Cache

# Singleton instance
indicator_cache = Cache(
    cache_name="indicators",
    max_size=500,  # Store up to 500 indicators
    max_age=2592000  # 30 days TTL
)


# Usage in indicator functions:
def calculate_rsi(prices, period=14, prices_hash=None):
    """Calculate RSI with caching."""

    # Generate cache key
    if prices_hash is None:
        prices_hash = hash_series(prices)  # SHA256 hash of price data

    cache_key = f"rsi_{prices_hash}_{period}"

    # Try to get from cache
    cached_rsi = indicator_cache.get(cache_key)

    if cached_rsi is not None:
        # Cache HIT - return immediately (no calculation)
        return cached_rsi

    # Cache MISS - calculate indicator
    delta = prices.diff()
    gain = delta.where(delta > 0, 0.0)
    loss = -delta.where(delta < 0, 0.0)

    avg_gain = gain.rolling(window=period).mean()
    avg_loss = loss.rolling(window=period).mean()

    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))

    # Store in cache for future use
    indicator_cache.set(cache_key, rsi)

    return rsi
```

**Cache Key**: `{indicator_name}_{data_hash}_{parameters}`

Example: `rsi_a3f5d2c1_14`

**Hash Generation**:

- SHA256 hash of price data content
- Same data produces same hash
- Different data produces different hash

**Example**:

```
Worker 1 processes ZS data:
  â”œâ”€ Test 1: RSI(period=14)
  â”‚   â”œâ”€ Hash ZS close prices: a3f5d2c1
  â”‚   â”œâ”€ Cache key: rsi_a3f5d2c1_14
  â”‚   â”œâ”€ Calculate RSI (cache MISS) â†’ Store in cache
  â”‚   â””â”€ Time: 5ms
  â”‚
  â”œâ”€ Test 2: RSI(period=14) on same ZS data
  â”‚   â”œâ”€ Same hash: a3f5d2c1
  â”‚   â”œâ”€ Cache key: rsi_a3f5d2c1_14
  â”‚   â”œâ”€ Cache HIT â†’ Return from memory
  â”‚   â””â”€ Time: 0.01ms
  â”‚
  â””â”€ Test 3: RSI(period=21) on same ZS data
      â”œâ”€ Same hash but different period
      â”œâ”€ Cache key: rsi_a3f5d2c1_21
      â”œâ”€ Calculate RSI (cache MISS) â†’ Store in cache
      â””â”€ Time: 5ms
```

### Cache Base Implementation

Both caches inherit from the same base class:

```python
# In cache/cache_base.py

class Cache:
    """
    LRU cache with file persistence and multi-process file locking.

    Features:
    â€¢ LRU eviction policy
    â€¢ File locking for concurrent access
    â€¢ TTL (time-to-live) expiration
    â€¢ Pickle serialization
    """

    def __init__(self, cache_name, max_size, max_age):
        self.cache_name = cache_name
        self.max_size = max_size
        self.max_age = max_age

        # File paths
        self.cache_file = Path(CACHE_DIR) / f"{cache_name}_cache.pkl"
        self.lock_file = Path(CACHE_DIR) / f"{cache_name}_cache.lock"

        # In-memory storage (OrderedDict for LRU)
        self.cache_data = OrderedDict()

        # Load from disk at initialization
        self._load_cache()

    def get(self, key):
        """Get value from cache (None if not found or expired)."""
        if key not in self.cache_data:
            return None

        timestamp, value = self.cache_data[key]

        # Check if expired
        if time.time() - timestamp > self.max_age:
            del self.cache_data[key]
            return None

        # Move to end (mark as recently used)
        self.cache_data.move_to_end(key)

        return value

    def set(self, key, value):
        """Add value to cache (with LRU eviction if needed)."""
        current_time = time.time()
        self.cache_data[key] = (current_time, value)
        self.cache_data.move_to_end(key)

        # Evict oldest if over size limit
        if len(self.cache_data) > self.max_size:
            self.cache_data.popitem(last=False)

    def save_cache(self):
        """Save cache to disk with file locking."""
        lock = FileLock(str(self.lock_file), timeout=60)
        with lock:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache_data, f)

    def _load_cache(self):
        """Load cache from disk with file locking."""
        if not self.cache_file.exists():
            return

        lock = FileLock(str(self.lock_file), timeout=10)
        with lock:
            with open(self.cache_file, 'rb') as f:
                self.cache_data = pickle.load(f)
```

**Implementation Details**:

1. **OrderedDict for LRU**:
    - Preserves insertion order
    - `move_to_end()` marks item as recently used
    - `popitem(last=False)` removes oldest item

2. **File Locking**:
    - Prevents corruption from concurrent writes
    - Uses `FileLock` library
    - Timeout to avoid deadlocks

3. **TTL with Lazy Expiration**:
    - Items expire after `max_age` seconds
    - Checked on `get()` operation
    - No background cleanup process

4. **Pickle Serialization**:
    - Handles complex Python objects
    - Binary format
    - Cross-version compatible

### Cache Synchronization Flow

```
Program Start:
  â”œâ”€ Main process loads caches from disk
  â”‚   â”œâ”€ dataframe_cache.pkl â†’ 30 entries
  â”‚   â””â”€ indicator_cache.pkl â†’ 500 entries
  â”‚
  â””â”€ Worker processes spawn
      â””â”€ Each worker loads same cache files
          â”œâ”€ Copy-on-write: Initially shares memory
          â””â”€ Becomes independent when modified

During Execution:
  â”œâ”€ Main process: Cache stays static (500 entries)
  â”‚   â””â”€ Main doesn't compute indicators
  â”‚
  â””â”€ Worker processes: Cache grows independently
      â”œâ”€ Worker 1: 500 â†’ 600 entries
      â”œâ”€ Worker 2: 500 â†’ 450 entries
      â””â”€ Worker N: 500 â†’ 520 entries
      â””â”€ âš ï¸ Updates isolated to each worker's memory

Program End:
  â”œâ”€ Workers terminate
  â”‚   â””â”€ Worker cache updates lost âŒ
  â”‚
  â””â”€ Main process saves cache
      â””â”€ indicator_cache.pkl â† Still 500 entries
      â””â”€ New calculations will be repeated next run
```

### File Locking for Multi-Process Safety

**Without Locking (Race Condition)**:
Main Process Worker Process
â”‚ â”‚
â”œâ”€ Read cache file â”‚
â”‚  (500 entries)                â”‚
â”‚ â”œâ”€ Read cache file
â”‚ â”‚  (500 entries)
â”œâ”€ Add entry â”‚
â”‚  (501 entries)                â”‚
â”‚ â”œâ”€ Add entry
â”‚ â”‚  (501 entries)
â”œâ”€ Write cache file â”€â”€â”€â”€â” â”‚
â”‚ âŒ Corrupted!         â”‚ â”œâ”€ Write cache file â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€ âŒ Corrupted!         â”‚
â”‚
Result: File garbled (invalid pickle) ðŸ’¥

```

**With Locking (Safe)**:

```

With FileLock:
Main Process Worker Process
â”‚ â”‚
â”œâ”€ Acquire lock âœ… â”‚
â”œâ”€ Read cache file â”‚
â”‚  (500 entries)                â”‚
â”œâ”€ Add entry â”‚
â”‚  (501 entries)                â”œâ”€ Try to acquire lock â³
â”œâ”€ Write cache file â”‚  (blocked, waiting...)
â”œâ”€ Release lock âœ… â”‚
â”‚ â”œâ”€ Acquire lock âœ…
â”‚ â”œâ”€ Read cache file
â”‚ â”‚  (501 entries) â† sees main's update
â”‚ â”œâ”€ Add entry
â”‚ â”‚  (502 entries)
â”‚ â”œâ”€ Write cache file
â”‚ â””â”€ Release lock âœ…

       Result: File intact, both updates preserved âœ…

```

**Implementation**:

```python
from filelock import FileLock

def save_cache(self):
    """Save cache with file locking."""
    try:
        lock = FileLock(str(self.lock_file), timeout=60)
        with lock:  # Blocks until lock acquired
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache_data, f)
        return True
    except Timeout:
        logger.error("Failed to acquire lock (timeout)")
        return False
```

**Lock File**:

- Separate `.lock` file for coordination
- NFS-safe (works across networked filesystems)
- Automatically released when `with` block exits
- Timeout prevents deadlocks

---

## Strategy Execution Pipeline

```
DataFrame (OHLC + Volume)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Add Indicators                                        â”‚
â”‚                                                               â”‚
â”‚  For each indicator function:                                 â”‚
â”‚    â”œâ”€> Generate cache key: (name, data_hash, params)         â”‚
â”‚    â”œâ”€> Check indicator_cache                                  â”‚
â”‚    â”‚   â””â”€> HIT: Return cached series (instant)               â”‚
â”‚    â”‚   â””â”€> MISS: Calculate indicator                         â”‚
â”‚    â”‚           â”œâ”€> Calculate using pandas operations          â”‚
â”‚    â”‚           â”œâ”€> Store in cache                             â”‚
â”‚    â”‚           â””â”€> Return calculated series                   â”‚
â”‚    â””â”€> Add column to DataFrame                                â”‚
â”‚                                                               â”‚
â”‚  Result: DataFrame with new columns:                          â”‚
â”‚    â””â”€> df['rsi'], df['ema_short'], df['macd'], etc.         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Generate Signals                                      â”‚
â”‚                                                               â”‚
â”‚  Strategy logic applied to indicators:                        â”‚
â”‚                                                               â”‚
â”‚  Example (RSI Strategy):                                      â”‚
â”‚    â”œâ”€> Detect RSI crossing below lower threshold (30)        â”‚
â”‚    â”‚   â””â”€> df['signal'] = 1 (BUY)                            â”‚
â”‚    â”‚                                                          â”‚
â”‚    â””â”€> Detect RSI crossing above upper threshold (70)        â”‚
â”‚        â””â”€> df['signal'] = -1 (SELL)                          â”‚
â”‚                                                               â”‚
â”‚  Result: DataFrame with 'signal' column:                      â”‚
â”‚    â””â”€> 1 = Long entry, -1 = Short entry, 0 = No action      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Extract Trades (Row-by-Row Iteration)                â”‚
â”‚                                                               â”‚
â”‚  For each row in DataFrame:                                   â”‚
â”‚    â”‚                                                          â”‚
â”‚    â”œâ”€> Check candle count                                    â”‚
â”‚    â”‚   â””â”€> Skip if count <= INDICATOR_WARMUP_PERIOD (100)   â”‚
â”‚    â”‚                                                          â”‚
â”‚    â”œâ”€> Handle Trailing Stop (if enabled)                     â”‚
â”‚    â”‚   â”œâ”€> Update trailing_stop based on high/low            â”‚
â”‚    â”‚   â””â”€> Close position if stop triggered                  â”‚
â”‚    â”‚                                                          â”‚
â”‚    â”œâ”€> Handle Contract Switch (futures rollover)             â”‚
â”‚    â”‚   â”œâ”€> Check if current_time >= next_switch              â”‚
â”‚    â”‚   â”œâ”€> Close position at prev bar's open                 â”‚
â”‚    â”‚   â””â”€> Reopen on new contract (if rollover=True)         â”‚
â”‚    â”‚                                                          â”‚
â”‚    â”œâ”€> Execute Queued Signal (from previous bar)             â”‚
â”‚    â”‚   â”œâ”€> If queued_signal == 1 and position != 1:         â”‚
â”‚    â”‚   â”‚   â”œâ”€> Close current position (if any)               â”‚
â”‚    â”‚   â”‚   â””â”€> Open LONG at current bar's open               â”‚
â”‚    â”‚   â”œâ”€> If queued_signal == -1 and position != -1:       â”‚
â”‚    â”‚   â”‚   â”œâ”€> Close current position (if any)               â”‚
â”‚    â”‚   â”‚   â””â”€> Open SHORT at current bar's open              â”‚
â”‚    â”‚   â””â”€> Reset queued_signal to None                       â”‚
â”‚    â”‚                                                          â”‚
â”‚    â””â”€> Queue New Signal (for next bar)                       â”‚
â”‚        â””â”€> If signal != 0: queued_signal = signal            â”‚
â”‚                                                               â”‚
â”‚  Result: List of trades                                       â”‚
â”‚    â””â”€> [{entry_time, entry_price, exit_time, exit_price,    â”‚
â”‚           side, switch}, ...]                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Calculate Metrics                                     â”‚
â”‚                                                               â”‚
â”‚  Per-Trade Metrics:                                           â”‚
â”‚    â”œâ”€> Load contract specifications                           â”‚
â”‚    â”œâ”€> Calculate points_gained                                â”‚
â”‚    â”œâ”€> Calculate dollar_return = points Ã— multiplier         â”‚
â”‚    â”œâ”€> Calculate return_pct_of_contract                       â”‚
â”‚    â”œâ”€> Calculate return_pct_of_margin                         â”‚
â”‚    â””â”€> Add commission costs                                   â”‚
â”‚                                                               â”‚
â”‚  Summary Metrics:                                             â”‚
â”‚    â”œâ”€> Basic: total_trades, wins, losses, win_rate           â”‚
â”‚    â”œâ”€> Returns: total_return, avg_return                      â”‚
â”‚    â”œâ”€> Risk: profit_factor, max_drawdown                      â”‚
â”‚    â””â”€> Advanced: Sharpe, Sortino, Calmar, VaR, ES            â”‚
â”‚                                                               â”‚
â”‚  Result: Metrics dictionary                                   â”‚
â”‚    â””â”€> {total_trades: 42, win_rate: 0.57, ...}              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Signal Queuing Deep Dive

### Why Signals Are Queued

In real-world trading, you cannot execute a trade at the exact moment a technical condition is met. There is always a
delay between:

1. Detecting the signal (e.g., at bar close)
2. Placing the order
3. Order execution (usually at next bar open)

### Implementation Timeline

```
Bar N (19:00 - 20:00):
â”œâ”€> CLOSE: 100.50
â”œâ”€> RSI crosses below 30 (at close)
â”œâ”€> Signal detected: BUY
â””â”€> Signal queued: self.queued_signal = 1

Bar N+1 (20:00 - 21:00):
â”œâ”€> OPEN: 101.00 â† Order executes here
â”œâ”€> Execute queued signal:
â”‚   â””â”€> Open LONG position at 101.00
â””â”€> Reset: self.queued_signal = None
```

### Code Flow

```python
# In _extract_trades() loop:

for idx, row in df.iterrows():
    signal = row['signal']  # 0, 1, or -1
    price_open = row['open']

    # Step 1: Execute queued signal from previous bar
    if self.queued_signal is not None:
        if self.queued_signal == 1 and self.position != 1:
            # Close current position if any
            if self.position is not None:
                self._close_position(idx, price_open)
            # Open LONG position
            self._open_new_position(1, idx, price_open)
        elif self.queued_signal == -1 and self.position != -1:
            # Close current position if any
            if self.position is not None:
                self._close_position(idx, price_open)
            # Open SHORT position
            self._open_new_position(-1, idx, price_open)

        self.queued_signal = None  # Reset after execution

    # Step 2: Queue new signal for next bar
    if signal != 0:
        self.queued_signal = signal
```

### Example Scenario (Real ZS Data)

```
Scenario: RSI Strategy on ZS Futures (Soybeans), lower=30, upper=70
Dataset: ZS 15-minute bars from 2025-02-04
Symbol: CBOT:ZS1! (Front Month Contract)
Price Format: Cents per bushel (e.g., 1055.75 = $10.5575/bushel)

Bar 108: 2025-02-04 11:00:00
  close=1055.25, RSI=32.4
  â””â”€> No signal (RSI > 30)

Bar 109: 2025-02-04 11:15:00
  close=1058.25, RSI=28.7 â† RSI crosses below 30
  â””â”€> Signal detected: BUY
  â””â”€> Queued: self.queued_signal = 1

Bar 110: 2025-02-04 11:30:00
  open=1058.50 â† Gap up from previous close (1058.25)
  â””â”€> Execute queued signal
  â””â”€> Open LONG at 1058.50 (not 1058.25!)
  â””â”€> This is realistic - markets gap between bars

Bar 118: 2025-02-04 13:00:00
  close=1057.25, RSI=71.2 â† RSI crosses above 70
  â””â”€> Signal detected: SELL
  â””â”€> Queued: self.queued_signal = -1

Bar 119: 2025-02-04 13:15:00
  open=1057.00 â† Gap down from previous close (1057.25)
  â””â”€> Execute queued signal
  â””â”€> Close LONG at 1057.00
  â””â”€> P&L: (1057.00 - 1058.50) Ã— 5000 bushels = -$75
  â””â”€> Open SHORT at 1057.00

Note: RSI values are illustrative. Actual RSI calculation requires previous bars.
```

## File Structure

```
app/backtesting/
â”œâ”€â”€ __init__.py                    # Main module exports
â”œâ”€â”€ strategy_factory.py            # Strategy creation and validation
â”œâ”€â”€ testing/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mass_tester.py            # Main orchestration
â”‚   â”œâ”€â”€ orchestrator.py           # Test coordination
â”‚   â”œâ”€â”€ runner.py                 # Single test runner
â”‚   â”œâ”€â”€ reporting.py              # Result reporting
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ dataframe_validators.py  # DataFrame validation
â”‚       â””â”€â”€ test_preparation.py      # Test setup utilities
â”œâ”€â”€ strategies/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_strategy.py      # Base class with trade extraction
â”‚   â”‚   â”œâ”€â”€ position_manager.py   # Position and slippage management
â”‚   â”‚   â”œâ”€â”€ trailing_stop_manager.py  # Trailing stop logic
â”‚   â”‚   â””â”€â”€ contract_switch_handler.py  # Contract rollover logic
â”‚   â”œâ”€â”€ rsi.py                    # RSI strategy implementation
â”‚   â”œâ”€â”€ ema.py                    # EMA crossover strategy
â”‚   â”œâ”€â”€ macd.py                   # MACD strategy
â”‚   â”œâ”€â”€ bollinger_bands.py        # Bollinger Bands strategy
â”‚   â””â”€â”€ ichimoku_cloud.py         # Ichimoku strategy
â”œâ”€â”€ indicators/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rsi.py                    # RSI calculation
â”‚   â”œâ”€â”€ ema.py                    # EMA calculation
â”‚   â”œâ”€â”€ macd.py                   # MACD calculation
â”‚   â”œâ”€â”€ bollinger_bands.py        # Bollinger Bands calculation
â”‚   â”œâ”€â”€ ichimoku_cloud.py         # Ichimoku Cloud calculation
â”‚   â””â”€â”€ atr.py                    # ATR calculation
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ per_trade_metrics.py      # Individual trade calculations
â”‚   â””â”€â”€ summary_metrics.py        # Aggregate statistics
â”œâ”€â”€ validators/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                   # Base validator class
â”‚   â”œâ”€â”€ common_validator.py       # Common parameter validation
â”‚   â”œâ”€â”€ constants.py              # Validation constants
â”‚   â”œâ”€â”€ rsi_validator.py          # RSI parameter validation
â”‚   â”œâ”€â”€ ema_validator.py          # EMA parameter validation
â”‚   â”œâ”€â”€ macd_validator.py         # MACD parameter validation
â”‚   â”œâ”€â”€ bollinger_validator.py    # Bollinger parameter validation
â”‚   â””â”€â”€ ichimoku_validator.py     # Ichimoku parameter validation
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ strategy_analyzer.py      # Result analysis and ranking
â”‚   â”œâ”€â”€ constants.py              # Analysis constants
â”‚   â”œâ”€â”€ data_helpers.py           # Data processing helpers
â”‚   â””â”€â”€ formatters.py             # Output formatting
â”œâ”€â”€ fetching/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_fetcher.py           # TradingView data fetching
â”‚   â””â”€â”€ validators.py             # Data validation
â””â”€â”€ cache/
    â”œâ”€â”€ cache_base.py             # Base cache class with LRU
    â”œâ”€â”€ dataframe_cache.py        # DataFrame caching
    â””â”€â”€ indicators_cache.py       # Indicator caching
```

## Configuration Constants

```python
# In testing/utils/test_preparation.py
MIN_ROWS_FOR_BACKTEST = 150  # Minimum DataFrame rows

# In strategies/base/base_strategy.py
INDICATOR_WARMUP_PERIOD = 100  # Candles to skip

# In cache/cache_base.py
DEFAULT_CACHE_MAX_SIZE = 1000  # Max cache items
DEFAULT_CACHE_MAX_AGE = 86400  # Cache expiration (seconds)
DEFAULT_CACHE_LOCK_TIMEOUT = 60  # File lock timeout
DEFAULT_CACHE_RETRY_ATTEMPTS = 3  # Save retry attempts

# In cache/indicators_cache.py
MAX_SIZE = 500  # Indicator cache size
MAX_AGE = 2592000  # 30 days

# In cache/dataframe_cache.py
MAX_SIZE = 50  # DataFrame cache size
MAX_AGE = 604800  # 7 days
```

## Error Handling

1. **Parameter Validation**: In strategy_factory
2. **DataFrame Validation**: Before strategy execution
3. **Metrics Validation**: After calculation
4. **Type Validation**: Before saving to parquet

### Logging Levels

- **ERROR**: Critical failures (missing required columns, file not found)
- **WARNING**: Data quality issues (NaN values, small datasets)
- **INFO**: Normal operation (no trades generated, test completion)

## Conclusion

The backtesting architecture uses parallel processing with intelligent caching to efficiently test thousands of strategy
variants while maintaining realistic signal execution and comprehensive validation for production trading decisions.
